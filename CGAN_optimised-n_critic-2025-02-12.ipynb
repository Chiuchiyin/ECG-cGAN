{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eacfae16-ec8a-4c07-87d9-71cf715a37e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3 s\n",
      "Wall time: 6.23 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "kaggle              NA\n",
       "keras               2.10.0\n",
       "matplotlib          3.8.0\n",
       "numpy               1.26.4\n",
       "pandas              2.1.4\n",
       "scipy               1.11.4\n",
       "session_info        1.0.0\n",
       "sklearn             1.2.2\n",
       "tensorflow          2.10.1\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "PIL                                         10.2.0\n",
       "aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42    NA\n",
       "absl                                        NA\n",
       "anyio                                       NA\n",
       "asttokens                                   NA\n",
       "astunparse                                  1.6.3\n",
       "attr                                        23.1.0\n",
       "attrs                                       23.1.0\n",
       "babel                                       2.11.0\n",
       "beta_ufunc                                  NA\n",
       "binom_ufunc                                 NA\n",
       "bleach                                      4.1.0\n",
       "botocore                                    1.31.64\n",
       "bottleneck                                  1.3.7\n",
       "brotli                                      1.0.9\n",
       "certifi                                     2024.07.04\n",
       "cffi                                        1.16.0\n",
       "chardet                                     4.0.0\n",
       "charset_normalizer                          2.0.4\n",
       "cloudpickle                                 2.2.1\n",
       "colorama                                    0.4.6\n",
       "comm                                        0.1.2\n",
       "cycler                                      0.10.0\n",
       "cython_runtime                              NA\n",
       "dateutil                                    2.8.2\n",
       "debugpy                                     1.6.7\n",
       "decorator                                   5.1.1\n",
       "defusedxml                                  0.7.1\n",
       "dill                                        0.3.7\n",
       "exceptiongroup                              1.2.0\n",
       "executing                                   0.8.3\n",
       "fastjsonschema                              NA\n",
       "flatbuffers                                 24.3.25\n",
       "fsspec                                      2023.10.0\n",
       "gast                                        NA\n",
       "google                                      NA\n",
       "h5py                                        3.9.0\n",
       "hypergeom_ufunc                             NA\n",
       "idna                                        3.4\n",
       "invgauss_ufunc                              NA\n",
       "ipykernel                                   6.28.0\n",
       "jedi                                        0.18.1\n",
       "jinja2                                      3.1.3\n",
       "joblib                                      1.2.0\n",
       "json5                                       NA\n",
       "jsonpointer                                 2.1\n",
       "jsonschema                                  4.19.2\n",
       "jsonschema_specifications                   NA\n",
       "jupyter_events                              0.8.0\n",
       "jupyter_server                              2.10.0\n",
       "jupyterlab_server                           2.25.1\n",
       "kiwisolver                                  1.4.4\n",
       "lz4                                         4.3.2\n",
       "markupsafe                                  2.1.3\n",
       "matplotlib_inline                           0.1.6\n",
       "mkl                                         2.4.0\n",
       "mpl_toolkits                                NA\n",
       "nbformat                                    5.9.2\n",
       "nbinom_ufunc                                NA\n",
       "ncf_ufunc                                   NA\n",
       "nct_ufunc                                   NA\n",
       "ncx2_ufunc                                  NA\n",
       "nt                                          NA\n",
       "numexpr                                     2.8.7\n",
       "opt_einsum                                  v3.3.0\n",
       "overrides                                   NA\n",
       "packaging                                   23.1\n",
       "parso                                       0.8.3\n",
       "pickleshare                                 0.7.5\n",
       "pkg_resources                               NA\n",
       "platformdirs                                3.10.0\n",
       "prometheus_client                           NA\n",
       "prompt_toolkit                              3.0.43\n",
       "psutil                                      5.9.0\n",
       "pure_eval                                   0.2.2\n",
       "pyarrow                                     14.0.2\n",
       "pydev_ipython                               NA\n",
       "pydevconsole                                NA\n",
       "pydevd                                      2.9.5\n",
       "pydevd_file_utils                           NA\n",
       "pydevd_plugins                              NA\n",
       "pydevd_tracing                              NA\n",
       "pydot                                       3.0.4\n",
       "pygments                                    2.15.1\n",
       "pyparsing                                   3.0.9\n",
       "pythoncom                                   NA\n",
       "pythonjsonlogger                            NA\n",
       "pytz                                        2023.3.post1\n",
       "pywin32_system32                            NA\n",
       "pywintypes                                  NA\n",
       "referencing                                 NA\n",
       "requests                                    2.31.0\n",
       "rfc3339_validator                           0.1.4\n",
       "rfc3986_validator                           0.1.1\n",
       "rpds                                        NA\n",
       "ruamel                                      NA\n",
       "send2trash                                  NA\n",
       "setuptools                                  69.2.0\n",
       "six                                         1.16.0\n",
       "skewnorm_ufunc                              NA\n",
       "slugify                                     5.0.2\n",
       "snappy                                      NA\n",
       "sniffio                                     1.3.0\n",
       "socks                                       1.7.1\n",
       "sphinxcontrib                               NA\n",
       "stack_data                                  0.2.0\n",
       "tblib                                       1.7.0\n",
       "tensorboard                                 2.10.1\n",
       "termcolor                                   NA\n",
       "text_unidecode                              NA\n",
       "threadpoolctl                               2.2.0\n",
       "tornado                                     6.3.3\n",
       "tqdm                                        4.65.0\n",
       "traitlets                                   5.7.1\n",
       "typing_extensions                           NA\n",
       "urllib3                                     2.0.7\n",
       "wcwidth                                     0.2.5\n",
       "webencodings                                0.5.1\n",
       "websocket                                   0.58.0\n",
       "win32api                                    NA\n",
       "win32com                                    NA\n",
       "win32con                                    NA\n",
       "win32trace                                  NA\n",
       "winerror                                    NA\n",
       "wrapt                                       1.14.1\n",
       "yaml                                        6.0.1\n",
       "zmq                                         25.1.2\n",
       "zoneinfo                                    NA\n",
       "zope                                        NA\n",
       "zstandard                                   0.19.0\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             8.20.0\n",
       "jupyter_client      8.6.0\n",
       "jupyter_core        5.5.0\n",
       "jupyterlab          4.0.11\n",
       "notebook            7.0.8\n",
       "-----\n",
       "Python 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-10-10.0.22631-SP0\n",
       "-----\n",
       "Session information updated at 2025-02-12 01:15\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import kaggle as kg\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import backend\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "warnings.simplefilter(action = 'ignore', category = Warning)\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "import winsound #This only work on windows, comment it out when working with other OS\n",
    "\n",
    "sys.version\n",
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c429358-d86a-4b17-bbcd-6c1673c255ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n",
      "GPU 0: NVIDIA GeForce RTX 3070 Laptop GPU (UUID: GPU-22e7aaac-adad-9a2f-120f-e4e0a027d5a6)\n",
      "Number of replicas: 1\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"GPU is available.\")\n",
    "        !nvidia-smi -L\n",
    "    else:\n",
    "        print(\"Training on CPU.\")\n",
    "\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fbed309-fc3c-449b-bb79-20fd6b2a06d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3070 Laptop GPU, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "#mixed precision to save resources use\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "#limits memory use. Kernel would crash if there is not enough avaliable memory. \n",
    "#Limiting it helps avoiding that at the cost of possible bottleneck.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7168)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895e1716-7102-4eda-adc9-93bbcab3ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "datestr = time.strftime(\"%Y%m%d\")\n",
    "ROOT_DIR = \"./\"\n",
    "DATA_DIR = \"%s/data\" % ROOT_DIR\n",
    "EVAL_DIR = \"%s/evaluation\" % ROOT_DIR\n",
    "MODEL_DIR = (\"%s/models/CGAN/\"+ datestr) % ROOT_DIR\n",
    "WEIGHTS_DIR = (\"%s/weights/CGAN/\"+ datestr) % ROOT_DIR\n",
    "SAMPLES_DIR = (\"%s/samples/CGAN/\"+ datestr) % ROOT_DIR\n",
    "GENERATED_DIR = \"%s/generated\" % ROOT_DIR\n",
    "\n",
    "#Create missing directories, if they don't exist\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(DATA_DIR)\n",
    "    print(\"The data directory is created!\")\n",
    "if not os.path.exists(EVAL_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(EVAL_DIR)\n",
    "    print(\"The evaluation directory is created!\")\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(MODEL_DIR)\n",
    "    print(\"The model directory is created!\")\n",
    "if not os.path.exists(WEIGHTS_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(WEIGHTS_DIR)\n",
    "    print(\"The weights directory is created!\")\n",
    "if not os.path.exists(SAMPLES_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(SAMPLES_DIR)\n",
    "    print(\"The samples directory is created!\")\n",
    "if not os.path.exists(GENERATED_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(GENERATED_DIR)\n",
    "    print(\"The generated directory is created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971cc21-84b4-4c49-9ca7-b1a9d4596424",
   "metadata": {},
   "source": [
    "%%time\n",
    "kg.api.dataset_download_files(dataset = \"shayanfazeli/heartbeat\", path= DATA_DIR, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3198c1-e260-4027-a9ba-7d63a85692d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.28 s\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_file_path = '%s/mitbih_train.csv' % DATA_DIR\n",
    "column_names = [f\"Point {i+1}\" for i in range(187)] + [\"label\"]\n",
    "df_train = pd.read_csv(train_file_path, names=column_names)\n",
    "df_train[\"label\"] = df_train[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce37c676-51b6-48fa-bc95-ab5f84e7b7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "img_rows = 187\n",
    "img_cols = 1\n",
    "channels = 1\n",
    "\n",
    "# Input image dimensions\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "# Size of the noise vector, used as input to the Generator\n",
    "z_dim = 256\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 5\n",
    "\n",
    "# frequency for winsound beep is set to 262Hz (approximately middle C), please note that winsound only work on windows.\n",
    "freq = 262\n",
    " \n",
    "# duration is set to 1.5 seconds             \n",
    "dur = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc196b28-5477-4648-9161-f99667c4da31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_generator(z_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Layer 1\n",
    "    model.add(Dense(512*4, input_dim=z_dim))\n",
    "    model.add(Reshape((4, 512)))\n",
    "    \n",
    "    #Layer 2\n",
    "    model.add(Conv1DTranspose(256, kernel_size=4, strides=2, padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "\n",
    "    #Layer 3\n",
    "    model.add(Conv1DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "    \n",
    "    #Layer 4\n",
    "    model.add(Conv1DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "\n",
    "    #Layer 5\n",
    "    model.add(Conv1DTranspose(1, kernel_size=4, strides=2, padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "\n",
    "    # Output layer with tanh activation\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(187))\n",
    "    model.add(Reshape((187, 1)))\n",
    "    model.add(Activation('tanh'))\n",
    "\n",
    "    return model\n",
    "#build_generator(z_dim).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8edd425f-3c4d-4551-9c72-79951d9eaa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_cgan_generator(z_dim):\n",
    "\n",
    "    # Random noise vector z\n",
    "    z = Input(shape=(z_dim, ))\n",
    "\n",
    "    # Conditioning label: integer 0-9 specifying the number G should generate\n",
    "    label = Input(shape=(1, ), dtype='int32')\n",
    "\n",
    "    # Label embedding:\n",
    "    # ----------------\n",
    "    # Turns labels into dense vectors of size z_dim\n",
    "    # Produces 3D tensor with shape (batch_size, 1, z_dim)\n",
    "    label_embedding = Embedding(num_classes, z_dim, input_length=1)(label)\n",
    "\n",
    "    # Flatten the embedding 3D tensor into 2D tensor with shape (batch_size, z_dim)\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "    # Element-wise product of the vectors z and the label embeddings\n",
    "    joined_representation = Multiply()([z, label_embedding])\n",
    "\n",
    "    generator = build_generator(z_dim)\n",
    "\n",
    "    # Generate image for the given label\n",
    "    conditioned_img = generator(joined_representation)\n",
    "\n",
    "    return Model([z, label], conditioned_img)\n",
    "#build_cgan_generator(z_dim).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57329dc0-32a7-4c0d-a9cf-fe795536b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_discriminator(img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape = (img_shape[0], img_shape[1], img_shape[2] + 1)))\n",
    "    \n",
    "    #Layer 1\n",
    "    model.add(\n",
    "        Conv1D(32,\n",
    "               kernel_size=4,\n",
    "               strides=2,\n",
    "               padding='same', use_bias=False))\n",
    "    \n",
    "    # Batch normalization\n",
    "    #model.add(BatchNormalization())\n",
    "    \n",
    "    # Leaky ReLU activation\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(\n",
    "        Conv1D(64,\n",
    "               kernel_size=4,\n",
    "               strides=2,\n",
    "               padding='same', use_bias=False))\n",
    "\n",
    "    # Batch normalization\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    # Leaky ReLU activation\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(\n",
    "        Conv1D(128,\n",
    "               kernel_size=4,\n",
    "               strides=2,\n",
    "               padding='same', use_bias=False))\n",
    "\n",
    "    # Batch normalization\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    # Leaky ReLU\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer with sigmoid activation\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "#build_discriminator(img_shape).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13abe20d-fa92-461f-9be9-b868bb487f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_cgan_discriminator(img_shape):\n",
    "\n",
    "    # Input image\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    # Label for the input image\n",
    "    label = Input(shape=(1, ), dtype='int32')\n",
    "\n",
    "    #print(\"Shape of img:\", img.shape)\n",
    "    #print(\"Shape of label:\", label.shape)\n",
    "\n",
    "    # Label embedding:\n",
    "    # ----------------\n",
    "    # Turns labels into dense vectors of size z_dim\n",
    "    # Produces 3D tensor with shape (batch_size, 1, 28*28*1)\n",
    "    label_embedding = Embedding(num_classes,\n",
    "                                np.prod(img_shape),\n",
    "                                input_length=1)(label)\n",
    "\n",
    "    # Flatten the embedding 3D tensor into 2D tensor with shape (batch_size, 28*28*1)\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "    # Reshape label embeddings to have same dimensions as input images\n",
    "    label_embedding = Reshape(img_shape)(label_embedding)\n",
    "\n",
    "    # Concatenate images with their label embeddings\n",
    "    concatenated = Concatenate(axis=-1)([img, label_embedding])\n",
    "\n",
    "    #print(\"Shape after concatenation:\", concatenated.shape)\n",
    "\n",
    "    discriminator = build_discriminator(img_shape)\n",
    "\n",
    "    # Classify the image-label pair\n",
    "    classification = discriminator(concatenated)\n",
    "\n",
    "    return Model([img, label], classification)\n",
    "#build_cgan_discriminator(img_shape).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d62c264-f92e-4d9a-a25e-1232a20c753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_cgan(generator, discriminator):\n",
    "\n",
    "    # Random noise vector z\n",
    "    z = Input(shape=(z_dim, ))\n",
    "\n",
    "    # Image label\n",
    "    label = Input(shape=(1, ))\n",
    "\n",
    "    # Generated image for that label\n",
    "    img = generator([z, label])\n",
    "\n",
    "    classification = discriminator([img, label])\n",
    "\n",
    "    # Combined Generator -> Discriminator model\n",
    "    # G([z, lablel]) = x*\n",
    "    # D(x*) = classification\n",
    "    model = Model([z, label], classification)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58511b8d-0a1b-4e99-9cb4-d3397e2614b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.52 s\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build and compile the Discriminator\n",
    "discriminator = build_cgan_discriminator(img_shape)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Build the Generator\n",
    "generator = build_cgan_generator(z_dim)\n",
    "\n",
    "# Keep Discriminatorâ€™s parameters constant for Generator training\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Build and compile CGAN model with fixed Discriminator to train the Generator\n",
    "cgan = build_cgan(generator, discriminator)\n",
    "cgan.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83eacb37-13a7-4514-8cc6-4bd723f2afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data, noise_level = 0.1):\n",
    "    \"\"\"\n",
    "    Adds noise to the non-padded region of the dataset, keeping data within [0, 1].\n",
    "    \n",
    "    Args:\n",
    "        ecg_signal (numpy.ndarray): The input dataset of shape (n_samples, n_features).\n",
    "                              Rows represent observations, columns represent time points.\n",
    "        noise_level (float): Standard deviation of the Gaussian noise.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Dataset with noise added to the non-padded regions, clipped to [0, 1].\n",
    "    \"\"\"\n",
    "    noisy_data = data.copy()\n",
    "    for i, row in enumerate(data):\n",
    "        # Find the last non-zero index\n",
    "        non_padded_length = np.max(np.nonzero(row)) + 1\n",
    "        if non_padded_length > 0:\n",
    "            # Add Gaussian noise to the non-padded region\n",
    "            noise = np.random.normal(0, noise_level, non_padded_length)\n",
    "            noisy_data[i, :non_padded_length] += noise\n",
    "            # Clip values to the range [0, 1]\n",
    "            noisy_data[i, :non_padded_length] = np.clip(noisy_data[i, :non_padded_length], 0, 1)\n",
    "    \n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec6bf56-48bf-4e2e-bd13-727c93308a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(df_real, df_generated):\n",
    "    \"\"\"\n",
    "    Compute FID score between two pandas dataframes where:\n",
    "    - Each row is a different signal\n",
    "    - Each column is a data point of the signal\n",
    "\n",
    "    Parameters:\n",
    "        df_real (pd.DataFrame): DataFrame of real signals\n",
    "        df_generated (pd.DataFrame): DataFrame of generated signals\n",
    "\n",
    "    Returns:\n",
    "        fid_score (float): Computed FID score\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure inputs are either NumPy arrays or Pandas DataFrames\n",
    "    if isinstance(df_real, np.ndarray):\n",
    "        real_data = df_real\n",
    "    elif hasattr(df_real, \"to_numpy\"):  # Handles Pandas DataFrame\n",
    "        real_data = df_real.to_numpy()\n",
    "    else:\n",
    "        raise TypeError(\"df_real must be a NumPy array or Pandas DataFrame\")\n",
    "\n",
    "    if isinstance(df_generated, np.ndarray):\n",
    "        gen_data = df_generated\n",
    "    elif hasattr(df_generated, \"to_numpy\"):\n",
    "        gen_data = df_generated.to_numpy()\n",
    "    else:\n",
    "        raise TypeError(\"df_generated must be a NumPy array or Pandas DataFrame\")\n",
    "\n",
    "    # Validate dimensions\n",
    "    if real_data.shape[1] != gen_data.shape[1]:\n",
    "        raise ValueError(\"Both datasets must have the same number of columns (features)\")\n",
    "\n",
    "    # Compute mean and covariance matrices\n",
    "    mu_real, sigma_real = np.mean(real_data, axis=0), np.cov(real_data, rowvar=False)\n",
    "    mu_gen, sigma_gen = np.mean(gen_data, axis=0), np.cov(gen_data, rowvar=False)\n",
    "\n",
    "    # Compute square root of product of covariances\n",
    "    cov_sqrt, _ = sqrtm(sigma_real @ sigma_gen, disp=False)\n",
    "\n",
    "    # Check for numerical issues (complex numbers)\n",
    "    if np.iscomplexobj(cov_sqrt):\n",
    "        cov_sqrt = cov_sqrt.real\n",
    "\n",
    "    # Compute FID score\n",
    "    fid_score = np.sum((mu_real - mu_gen) ** 2) + np.trace(sigma_real + sigma_gen - 2 * cov_sqrt)\n",
    "    \n",
    "    return fid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08eb54ba-f064-4c2f-8c67-4a7cb11132ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accuracies = []\n",
    "losses = []\n",
    "#fid_scores = []\n",
    "n_critic = []\n",
    "\n",
    "def train(iterations, batch_size, sample_interval, n_discriminator = 1):\n",
    "\n",
    "    print(\"Target number of iteration: \" + str(iterations))\n",
    "    print(\"Initial number of discriminator: \" + str(n_discriminator))\n",
    "\n",
    "    # Labels for real data\n",
    "    real = np.ones((batch_size, 1))\n",
    "\n",
    "    # Labels for fake data\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    sample_number = math.ceil(batch_size/5)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        n_critic.append(n_discriminator)\n",
    "\n",
    "        # -------------------------\n",
    "        #  Train the Discriminator\n",
    "        # -------------------------\n",
    "\n",
    "        for x in range(n_discriminator):\n",
    "\n",
    "            # Generate labels\n",
    "            # Labels for real data\n",
    "            #real = np.random.uniform(0.9, 1.0, size=(batch_size, 1))\n",
    "            \n",
    "            # Labels for fake data\n",
    "            #fake = np.random.uniform(0, 0.1, size=(batch_size, 1))\n",
    "            \n",
    "            # Sampling real data\n",
    "            df_sample=df_train.groupby('label', group_keys=False).sample(n=math.ceil(batch_size/5))\n",
    "            df_sample = df_sample.sample(frac=1)\n",
    "\n",
    "            (X_train, y_train) = (df_sample.drop(['label'], axis=1).values, df_sample[\"label\"].values)\n",
    "\n",
    "            X_train = augment_data(X_train, noise_level = 0.001)\n",
    "            # Rescale values to [-1, 1]\n",
    "            X_train = 2*X_train - 1.\n",
    "            X_train = np.expand_dims(X_train, axis=2)\n",
    "\n",
    "            # Get real batch\n",
    "            imgs, labels = X_train, y_train\n",
    "\n",
    "            # Generate fake batch\n",
    "            z = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "            gen_imgs = generator.predict([z, y_train], verbose=0)\n",
    "\n",
    "            # Train on real + fake data\n",
    "            d_loss_real = discriminator.train_on_batch([X_train, y_train], real)\n",
    "            d_loss_fake = discriminator.train_on_batch([gen_imgs, y_train], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # End of discriminator training loop\n",
    "\n",
    "        # Check for discriminator output range\n",
    "        fake_real = discriminator.predict([gen_imgs, y_train], verbose = 0)\n",
    "\n",
    "        max_value = max(fake_real)\n",
    "        min_value = min(fake_real)\n",
    "            \n",
    "        # Conditions to modify discriminator training frequency             \n",
    "        if max_value < 0.7 and min_value > 0.3:\n",
    "            n_discriminator = n_discriminator+1\n",
    "            print(f\"\\nIteration {iteration+1}/{iterations}\", file=sys.__stdout__)\n",
    "            print(f\"D Acc Real: {100*d_loss_real[1]:.2f}% | D Acc Fake: {100*d_loss_fake[1]:.2f}% | D Acc: {100*d_loss[1]:.2f}%\", file=sys.__stdout__)\n",
    "            print(min_value, max_value, file=sys.__stdout__)\n",
    "            print(\"discriminator might have difficulty differentiating real and fake\", file=sys.__stdout__)\n",
    "            print(\"increasing the number of discriminator loop\", file=sys.__stdout__)\n",
    "        \n",
    "        elif d_loss[1] < 0.3:\n",
    "            n_discriminator = n_discriminator+1\n",
    "            print(f\"\\nIteration {iteration+1}/{iterations}\", file=sys.__stdout__)\n",
    "            print(f\"D Acc Real: {100*d_loss_real[1]:.2f}% | D Acc Fake: {100*d_loss_fake[1]:.2f}% | D Acc: {100*d_loss[1]:.2f}%\", file=sys.__stdout__)\n",
    "            print(\"discriminator accuracy is dangerously low\", file=sys.__stdout__)\n",
    "            print(min_value, max_value, file=sys.__stdout__)\n",
    "            print(\"increasing the number of discriminator loop\", file=sys.__stdout__)\n",
    "\n",
    "        elif n_discriminator > 1 and d_loss[1] > 0.7:\n",
    "            n_discriminator = n_discriminator-1\n",
    "            print(f\"\\nIteration {iteration+1}/{iterations}\", file=sys.__stdout__)\n",
    "            print(f\"D Acc Real: {100*d_loss_real[1]:.2f}% | D Acc Fake: {100*d_loss_fake[1]:.2f}% | D Acc: {100*d_loss[1]:.2f}%\", file=sys.__stdout__)\n",
    "            print(\"discriminator accuracy is too high\", file=sys.__stdout__)\n",
    "            print(min_value, max_value, file=sys.__stdout__)\n",
    "            print(\"decreasing the number of discriminator loop\", file=sys.__stdout__)                   \n",
    "\n",
    "        # ---------------------\n",
    "        #  Train the Generator\n",
    "        # ---------------------\n",
    "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "        labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
    "        g_loss = cgan.train_on_batch([z, y_train], np.ones((batch_size, 1)))\n",
    "\n",
    "        losses.append((d_loss[0], g_loss))\n",
    "        accuracies.append(100 * d_loss[1])\n",
    "            \n",
    "        if (iteration + 1) % sample_interval == 0 or (iteration + 1) == iterations:\n",
    "            timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            print(f\"\\n{timestr}\", file=sys.__stdout__)\n",
    "            print(f\"Iteration {iteration+1}/{iterations}\", file=sys.__stdout__)\n",
    "            print(f\"D Loss: {d_loss[0]:.4f} | G Loss: {g_loss:.4f} | \"\n",
    "                  f\"D Acc Real: {100*d_loss_real[1]:.2f}% | D Acc Fake: {100*d_loss_fake[1]:.2f}% | D Acc: {100*d_loss[1]:.2f}%\", file=sys.__stdout__)\n",
    "            name = str('%s/'+ timestr + '_CGAN_generator_' + str(iteration+1) + '.keras') % MODEL_DIR\n",
    "            generator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "            generator.save(name)\n",
    "            name = str('%s/'+ timestr + '_CGAN_discriminator_' + str(iteration+1) + '.keras') % MODEL_DIR\n",
    "            discriminator.save(name)\n",
    "            name = str('%s/'+ timestr + '_CGAN_gan_' + str(iteration+1) + '.keras') % MODEL_DIR\n",
    "            cgan.save(name)\n",
    "            name = str('%s/'+ timestr + '_CGAN_generator_' + str(iteration+1) + '.weights.h5') % WEIGHTS_DIR\n",
    "            generator.save_weights(name)\n",
    "            name = str('%s/'+ timestr + '_CGAN_discriminator_' + str(iteration+1) + '.weights.h5') % WEIGHTS_DIR\n",
    "            discriminator.save_weights(name)\n",
    "            name = str('%s/'+ timestr + '_CGAN_gan_' + str(iteration+1) + '.weights.h5') % WEIGHTS_DIR\n",
    "            cgan.save_weights(name)\n",
    "            \n",
    "            name = str('%s/'+ timestr + 'losses_CGAN' + str(iteration+1)) % MODEL_DIR\n",
    "            np.save(name, losses)\n",
    "            name = str('%s/'+ timestr + 'accuracy_CGAN' + str(iteration+1)) % MODEL_DIR\n",
    "            np.save(name, accuracies)\n",
    "\n",
    "            sample_images(iteration)\n",
    "            \n",
    "    print(\"We have reached the end of training\")\n",
    "    print(\"Number of Discriminator at the end is: \", n_discriminator)\n",
    "    winsound.Beep(freq, dur)\n",
    "    winsound.Beep(freq*2, dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dec366e-7071-4529-9a92-618cfcce47c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def sample_images(iteration,image_grid_rows=10, image_grid_columns=5):\n",
    "\n",
    "    # Sample random noise\n",
    "    z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n",
    "\n",
    "    # Get image labels 0-4\n",
    "    label = np.arange(0, 5)\n",
    "    labels = np.tile(label, image_grid_rows).reshape(-1, 1)\n",
    "\n",
    "    # Generate images from random noise\n",
    "    gen_imgs = generator.predict([z, labels], verbose=0)\n",
    "\n",
    "    # Rescale image pixel values to [0, 1]\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    # Set image grid\n",
    "    fig, axs = plt.subplots(image_grid_rows,\n",
    "                            image_grid_columns,\n",
    "                            figsize=(20, 35))\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(image_grid_rows):\n",
    "        for j in range(image_grid_columns):\n",
    "            # Output a grid of images\n",
    "            axs[i, j].plot(gen_imgs[cnt, :].reshape(-1,))\n",
    "            axs[i, j].set_title(\"label: %d\" % labels[cnt])\n",
    "            cnt += 1\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    name = str('%s/'+ timestr + '_CGAN_' + str(iteration+1) + '.png') % SAMPLES_DIR\n",
    "    fig.savefig(name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38358e42-d3a0-4350-9d0d-896cb0164731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set hyperparameters\n",
    "iterations = 2000\n",
    "batch_size = 200\n",
    "sample_interval = 100\n",
    "n_discriminator = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d224e5c-62b4-427c-86ef-25c0ca437343",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_file_path = \"%s/../20250211/20250211-041520_CGAN_generator_1000.weights.h5\" % WEIGHTS_DIR\n",
    "generator.load_weights(generator_file_path, skip_mismatch=False, by_name=False, options=None)\n",
    "#generator = tf.keras.models.load_model(generator_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3740bc2-4062-431d-b10b-b8133f03857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_file_path = \"%s/../20250211/20250211-041520_CGAN_discriminator_1000.weights.h5\" % WEIGHTS_DIR\n",
    "discriminator.load_weights(discriminator_file_path, skip_mismatch=False, by_name=False, options=None)\n",
    "#discriminator = tf.keras.models.load_model(discriminator_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6012f19b-6879-4b86-9af3-d6b059fb1452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target number of iteration: 2000\n",
      "Initial number of discriminator: 53\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the CGAN for the specified number of iterations\n",
    "train(iterations, batch_size, sample_interval, n_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad29bd-6e6b-4f05-9972-cb19f814c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(freq*2, dur)\n",
    "winsound.Beep(freq, dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d275949-b538-42f6-8ff3-e1464d38012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss = np.array(losses)\n",
    "\n",
    "# Plot training losses for Discriminator and Generator\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(loss.T[0], label=\"Discriminator loss\")\n",
    "plt.plot(loss.T[1], label=\"Generator loss\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e9ee5-c5a1-4747-bf51-c64359d107c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "acc = np.array(accuracies)\n",
    "\n",
    "# Plot Discriminator accuracy\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(acc, label=\"Discriminator accuracy\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(range(0, 100, 5))\n",
    "\n",
    "plt.title(\"Discriminator Accuracy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970bd04-6ef2-4a31-90c3-8eb0ee8b1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#acc = np.array(accuracies)\n",
    "\n",
    "# Plot Discriminator accuracy\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(n_critic, label=\"Discriminator training loop\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.title(\"No of discriminator training loop\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"No of loop\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63b355-2220-4ed8-b717-8689db14b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_critic[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0818c-e62c-4348-b32e-ab386a87823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feae0c1-1580-45ef-924f-c2905a71ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = str('%s/' + 'n_critic_CGAN' + str(len(n_critic))) % MODEL_DIR\n",
    "np.save(name, n_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f5900-971b-4e0d-960c-d9a4323ae039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heart-beat",
   "language": "python",
   "name": "heart-beat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
