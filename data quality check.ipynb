{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334a9d7e-121b-4686-aa2e-d0d86932eb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "keras               2.10.0\n",
       "matplotlib          3.8.0\n",
       "numpy               1.26.4\n",
       "pandas              2.1.4\n",
       "scipy               1.11.4\n",
       "seaborn             0.12.2\n",
       "session_info        1.0.0\n",
       "tensorflow          2.10.1\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "PIL                                         10.2.0\n",
       "aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42    NA\n",
       "absl                                        NA\n",
       "anyio                                       NA\n",
       "asttokens                                   NA\n",
       "astunparse                                  1.6.3\n",
       "attr                                        23.1.0\n",
       "attrs                                       23.1.0\n",
       "babel                                       2.11.0\n",
       "beta_ufunc                                  NA\n",
       "binom_ufunc                                 NA\n",
       "botocore                                    1.31.64\n",
       "bottleneck                                  1.3.7\n",
       "brotli                                      1.0.9\n",
       "certifi                                     2024.07.04\n",
       "cffi                                        1.16.0\n",
       "chardet                                     4.0.0\n",
       "charset_normalizer                          2.0.4\n",
       "cloudpickle                                 2.2.1\n",
       "colorama                                    0.4.6\n",
       "comm                                        0.1.2\n",
       "cycler                                      0.10.0\n",
       "cython_runtime                              NA\n",
       "dateutil                                    2.8.2\n",
       "debugpy                                     1.6.7\n",
       "decorator                                   5.1.1\n",
       "defusedxml                                  0.7.1\n",
       "dill                                        0.3.7\n",
       "exceptiongroup                              1.2.0\n",
       "executing                                   0.8.3\n",
       "fastjsonschema                              NA\n",
       "flatbuffers                                 24.3.25\n",
       "fsspec                                      2023.10.0\n",
       "gast                                        NA\n",
       "google                                      NA\n",
       "h5py                                        3.9.0\n",
       "hypergeom_ufunc                             NA\n",
       "idna                                        3.4\n",
       "invgauss_ufunc                              NA\n",
       "ipykernel                                   6.28.0\n",
       "ipython_genutils                            0.2.0\n",
       "ipywidgets                                  7.6.5\n",
       "jedi                                        0.18.1\n",
       "jinja2                                      3.1.3\n",
       "json5                                       NA\n",
       "jsonpointer                                 2.1\n",
       "jsonschema                                  4.19.2\n",
       "jsonschema_specifications                   NA\n",
       "jupyter_events                              0.8.0\n",
       "jupyter_server                              2.10.0\n",
       "jupyterlab_server                           2.25.1\n",
       "kiwisolver                                  1.4.4\n",
       "lz4                                         4.3.2\n",
       "markupsafe                                  2.1.3\n",
       "mkl                                         2.4.0\n",
       "mpl_toolkits                                NA\n",
       "nbformat                                    5.9.2\n",
       "nbinom_ufunc                                NA\n",
       "ncf_ufunc                                   NA\n",
       "nct_ufunc                                   NA\n",
       "ncx2_ufunc                                  NA\n",
       "nt                                          NA\n",
       "numexpr                                     2.8.7\n",
       "opt_einsum                                  v3.3.0\n",
       "overrides                                   NA\n",
       "packaging                                   23.1\n",
       "parso                                       0.8.3\n",
       "patsy                                       0.5.3\n",
       "pickleshare                                 0.7.5\n",
       "pkg_resources                               NA\n",
       "platformdirs                                3.10.0\n",
       "prometheus_client                           NA\n",
       "prompt_toolkit                              3.0.43\n",
       "psutil                                      5.9.0\n",
       "pure_eval                                   0.2.2\n",
       "pyarrow                                     14.0.2\n",
       "pydev_ipython                               NA\n",
       "pydevconsole                                NA\n",
       "pydevd                                      2.9.5\n",
       "pydevd_file_utils                           NA\n",
       "pydevd_plugins                              NA\n",
       "pydevd_tracing                              NA\n",
       "pydot                                       3.0.4\n",
       "pygments                                    2.15.1\n",
       "pyparsing                                   3.0.9\n",
       "pythoncom                                   NA\n",
       "pythonjsonlogger                            NA\n",
       "pytz                                        2023.3.post1\n",
       "pywintypes                                  NA\n",
       "referencing                                 NA\n",
       "requests                                    2.31.0\n",
       "rfc3339_validator                           0.1.4\n",
       "rfc3986_validator                           0.1.1\n",
       "rpds                                        NA\n",
       "ruamel                                      NA\n",
       "send2trash                                  NA\n",
       "setuptools                                  69.2.0\n",
       "six                                         1.16.0\n",
       "skewnorm_ufunc                              NA\n",
       "snappy                                      NA\n",
       "sniffio                                     1.3.0\n",
       "socks                                       1.7.1\n",
       "sphinxcontrib                               NA\n",
       "stack_data                                  0.2.0\n",
       "statsmodels                                 0.14.0\n",
       "tblib                                       1.7.0\n",
       "tensorboard                                 2.10.1\n",
       "termcolor                                   NA\n",
       "tornado                                     6.3.3\n",
       "traitlets                                   5.7.1\n",
       "typing_extensions                           NA\n",
       "urllib3                                     2.0.7\n",
       "wcwidth                                     0.2.5\n",
       "websocket                                   0.58.0\n",
       "win32api                                    NA\n",
       "win32com                                    NA\n",
       "win32con                                    NA\n",
       "win32trace                                  NA\n",
       "winerror                                    NA\n",
       "wrapt                                       1.14.1\n",
       "yaml                                        6.0.1\n",
       "zmq                                         25.1.2\n",
       "zoneinfo                                    NA\n",
       "zope                                        NA\n",
       "zstandard                                   0.19.0\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             8.20.0\n",
       "jupyter_client      8.6.0\n",
       "jupyter_core        5.5.0\n",
       "jupyterlab          4.0.11\n",
       "notebook            7.0.8\n",
       "-----\n",
       "Python 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-10-10.0.22631-SP0\n",
       "-----\n",
       "Session information updated at 2025-02-04 14:41\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import backend\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "warnings.simplefilter(action = 'ignore', category = Warning)\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "sys.version\n",
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f99795-1c9f-48fa-9e2d-18038b0749e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n",
      "GPU 0: NVIDIA GeForce RTX 3070 Laptop GPU (UUID: GPU-22e7aaac-adad-9a2f-120f-e4e0a027d5a6)\n",
      "Number of replicas: 1\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"GPU is available.\")\n",
    "        !nvidia-smi -L\n",
    "    else:\n",
    "        print(\"Training on CPU.\")\n",
    "\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bfefdf-bcb7-458b-a245-2de0efac90f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3070 Laptop GPU, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "#mixed precision to save resources use\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "#limits memory use. Kernel would crash if there is not enough avaliable memory. \n",
    "#Limiting it helps avoiding that at the cost of possible bottleneck.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7168)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d05a34-61f3-4967-94bd-b2769ed4e0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model directory is created!\n",
      "The weights directory is created!\n",
      "The samples directory is created!\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 2.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "datestr = time.strftime(\"%Y%m%d\")\n",
    "ROOT_DIR = \"./\"\n",
    "DATA_DIR = \"%s/data\" % ROOT_DIR\n",
    "EVAL_DIR = \"%s/evaluation\" % ROOT_DIR\n",
    "MODEL_DIR = (\"%s/models/CGAN/\"+ datestr) % ROOT_DIR\n",
    "WEIGHTS_DIR = (\"%s/weights/CGAN/\"+ datestr) % ROOT_DIR\n",
    "SAMPLES_DIR = (\"%s/samples/CGAN/\"+ datestr) % ROOT_DIR\n",
    "GENERATED_DIR = \"%s/generated\" % ROOT_DIR\n",
    "\n",
    "#Create missing directories, if they don't exist\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(DATA_DIR)\n",
    "    print(\"The data directory is created!\")\n",
    "if not os.path.exists(EVAL_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(EVAL_DIR)\n",
    "    print(\"The evaluation directory is created!\")\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(MODEL_DIR)\n",
    "    print(\"The model directory is created!\")\n",
    "if not os.path.exists(WEIGHTS_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(WEIGHTS_DIR)\n",
    "    print(\"The weights directory is created!\")\n",
    "if not os.path.exists(SAMPLES_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(SAMPLES_DIR)\n",
    "    print(\"The samples directory is created!\")\n",
    "if not os.path.exists(GENERATED_DIR):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(GENERATED_DIR)\n",
    "    print(\"The generated directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b7e7aee-1ebf-4d14-b411-ff04394c0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_signal(row, window_length=11, polyorder=5):\n",
    "    return savgol_filter(row, window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "def calculate_fid(df_real, df_generated):\n",
    "    \"\"\"\n",
    "    Compute FID score between two pandas dataframes where:\n",
    "    - Each row is a different signal\n",
    "    - Each column is a data point of the signal\n",
    "\n",
    "    Parameters:\n",
    "        df_real (pd.DataFrame): DataFrame of real signals\n",
    "        df_generated (pd.DataFrame): DataFrame of generated signals\n",
    "\n",
    "    Returns:\n",
    "        fid_score (float): Computed FID score\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure inputs are either NumPy arrays or Pandas DataFrames\n",
    "    if isinstance(df_real, np.ndarray):\n",
    "        real_data = df_real\n",
    "    elif hasattr(df_real, \"to_numpy\"):  # Handles Pandas DataFrame\n",
    "        real_data = df_real.to_numpy()\n",
    "    else:\n",
    "        raise TypeError(\"df_real must be a NumPy array or Pandas DataFrame\")\n",
    "\n",
    "    if isinstance(df_generated, np.ndarray):\n",
    "        gen_data = df_generated\n",
    "    elif hasattr(df_generated, \"to_numpy\"):\n",
    "        gen_data = df_generated.to_numpy()\n",
    "    else:\n",
    "        raise TypeError(\"df_generated must be a NumPy array or Pandas DataFrame\")\n",
    "\n",
    "    # Validate dimensions\n",
    "    if real_data.shape[1] != gen_data.shape[1]:\n",
    "        raise ValueError(\"Both datasets must have the same number of columns (features)\")\n",
    "\n",
    "    # Compute mean and covariance matrices\n",
    "    mu_real, sigma_real = np.mean(real_data, axis=0), np.cov(real_data, rowvar=False)\n",
    "    mu_gen, sigma_gen = np.mean(gen_data, axis=0), np.cov(gen_data, rowvar=False)\n",
    "\n",
    "    # Compute square root of product of covariances\n",
    "    cov_sqrt, _ = sqrtm(sigma_real @ sigma_gen, disp=False)\n",
    "\n",
    "    # Check for numerical issues (complex numbers)\n",
    "    if np.iscomplexobj(cov_sqrt):\n",
    "        cov_sqrt = cov_sqrt.real\n",
    "\n",
    "    # Compute FID score\n",
    "    fid_score = np.sum((mu_real - mu_gen) ** 2) + np.trace(sigma_real + sigma_gen - 2 * cov_sqrt)\n",
    "    \n",
    "    return fid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6b6f1-81d7-4fae-ade7-33cd1299ec1c",
   "metadata": {},
   "source": [
    "load_file_path = '%s/' % GENERATED_DIR\n",
    "df_generated = pd.read_csv(load_file_path+\"20250201-150023_CGAN_generator_120000.csv\", encoding='utf-8', index_col=0)\n",
    "column_names = [f\"Point {i+1}\" for i in range(187)] + [\"label\"]\n",
    "df_generated.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4e01b4-336f-4f26-9ff4-467294102940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.59 s\n",
      "Wall time: 3.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_file_path = '%s/mitbih_train.csv' % DATA_DIR\n",
    "test_file_path = '%s/mitbih_test.csv' % DATA_DIR\n",
    "column_names = [f\"Point {i+1}\" for i in range(187)] + [\"label\"]\n",
    "df_train = pd.read_csv(train_file_path, names=column_names)\n",
    "df_test = pd.read_csv(test_file_path, names=column_names)\n",
    "df_train[\"label\"] = df_train[\"label\"].astype(int)\n",
    "df_test[\"label\"] = df_test[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621deb12-e33f-4cd5-bc17-1e90cffe3561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "img_rows = 187\n",
    "img_cols = 1\n",
    "channels = 1\n",
    "\n",
    "# Input image dimensions\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "# Size of the noise vector, used as input to the Generator\n",
    "z_dim = 256\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838fc904-f4d3-408b-b5bb-d164c587461e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_generator(z_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Layer 1\n",
    "    model.add(Dense(512*4, input_dim=z_dim))\n",
    "    model.add(Reshape((4, 512)))\n",
    "    \n",
    "    #Layer 2\n",
    "    model.add(Conv1DTranspose(256, kernel_size=4, strides=2, padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "\n",
    "    #Layer 3\n",
    "    model.add(Conv1DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "    \n",
    "    #Layer 4\n",
    "    model.add(Conv1DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "\n",
    "    #Layer 5\n",
    "    model.add(Conv1DTranspose(1, kernel_size=4, strides=2, padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "\n",
    "    # Output layer with tanh activation\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(187))\n",
    "    model.add(Reshape((187, 1)))\n",
    "    model.add(Activation('tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d356546f-7454-4eb1-9374-00a2ce4c3b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_cgan_generator(z_dim):\n",
    "\n",
    "    # Random noise vector z\n",
    "    z = Input(shape=(z_dim, ))\n",
    "\n",
    "    # Conditioning label: integer 0-9 specifying the number G should generate\n",
    "    label = Input(shape=(1, ), dtype='int32')\n",
    "\n",
    "    # Label embedding:\n",
    "    # ----------------\n",
    "    # Turns labels into dense vectors of size z_dim\n",
    "    # Produces 3D tensor with shape (batch_size, 1, z_dim)\n",
    "    label_embedding = Embedding(num_classes, z_dim, input_length=1)(label)\n",
    "\n",
    "    # Flatten the embedding 3D tensor into 2D tensor with shape (batch_size, z_dim)\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "    # Element-wise product of the vectors z and the label embeddings\n",
    "    joined_representation = Multiply()([z, label_embedding])\n",
    "\n",
    "    generator = build_generator(z_dim)\n",
    "\n",
    "    # Generate image for the given label\n",
    "    conditioned_img = generator(joined_representation)\n",
    "\n",
    "    return Model([z, label], conditioned_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59626f39-0d0b-4995-8b09-62e45d80779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_discriminator(img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape = (img_shape[0], img_shape[1], img_shape[2] + 1)))\n",
    "    \n",
    "    #Layer 1\n",
    "    model.add(\n",
    "        Conv1D(32,\n",
    "               kernel_size=4,\n",
    "               strides=2,\n",
    "               padding='same', use_bias=False))\n",
    "    \n",
    "    # Batch normalization\n",
    "    #model.add(BatchNormalization())\n",
    "    \n",
    "    # Leaky ReLU activation\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(\n",
    "        Conv1D(64,\n",
    "               kernel_size=4,\n",
    "               strides=2,\n",
    "               padding='same', use_bias=False))\n",
    "\n",
    "    # Batch normalization\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    # Leaky ReLU activation\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(\n",
    "        Conv1D(128,\n",
    "               kernel_size=4,\n",
    "               strides=2,\n",
    "               padding='same', use_bias=False))\n",
    "\n",
    "    # Batch normalization\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    # Leaky ReLU\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer with sigmoid activation\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14cf2ae8-f72d-49ad-9b01-e7da3659be02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_cgan_discriminator(img_shape):\n",
    "\n",
    "    # Input image\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    # Label for the input image\n",
    "    label = Input(shape=(1, ), dtype='int32')\n",
    "\n",
    "    #print(\"Shape of img:\", img.shape)\n",
    "    #print(\"Shape of label:\", label.shape)\n",
    "\n",
    "    # Label embedding:\n",
    "    # ----------------\n",
    "    # Turns labels into dense vectors of size z_dim\n",
    "    # Produces 3D tensor with shape (batch_size, 1, 28*28*1)\n",
    "    label_embedding = Embedding(num_classes,\n",
    "                                np.prod(img_shape),\n",
    "                                input_length=1)(label)\n",
    "\n",
    "    # Flatten the embedding 3D tensor into 2D tensor with shape (batch_size, 28*28*1)\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "    # Reshape label embeddings to have same dimensions as input images\n",
    "    label_embedding = Reshape(img_shape)(label_embedding)\n",
    "\n",
    "    # Concatenate images with their label embeddings\n",
    "    concatenated = Concatenate(axis=-1)([img, label_embedding])\n",
    "\n",
    "    #print(\"Shape after concatenation:\", concatenated.shape)\n",
    "\n",
    "    discriminator = build_discriminator(img_shape)\n",
    "\n",
    "    # Classify the image-label pair\n",
    "    classification = discriminator(concatenated)\n",
    "\n",
    "    return Model([img, label], classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c441bc37-ff48-4bd3-a17e-73c2d58b1292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.17 s\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build and compile the Discriminator\n",
    "discriminator = build_cgan_discriminator(img_shape)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Build the Generator\n",
    "generator = build_cgan_generator(z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0742a4ad-b532-436c-9410-691007c84bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_file_path = \"%s/../20250201/20250201-150023_CGAN_generator_120000.weights.h5\" % WEIGHTS_DIR\n",
    "generator.load_weights(generator_file_path, skip_mismatch=False, by_name=False, options=None)\n",
    "#generator = tf.keras.models.load_model(generator_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37f1b2b0-1af8-401d-9fcd-b505d687014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_file_path = \"%s/../20250201/20250201-150023_CGAN_discriminator_120000.weights.h5\" % WEIGHTS_DIR\n",
    "discriminator.load_weights(discriminator_file_path, skip_mismatch=False, by_name=False, options=None)\n",
    "#discriminator = tf.keras.models.load_model(discriminator_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c32e77-37e9-4d2d-bcec-f9f5ab9931a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def sample_data(amount):\n",
    "\n",
    "    # Sample random noise\n",
    "    z = np.random.normal(0, 1, (amount * 5, z_dim))\n",
    "\n",
    "    # Get image labels 0-4\n",
    "    label = np.arange(0, 5)\n",
    "    labels = np.tile(label, amount).reshape(-1, 1)\n",
    "\n",
    "    # Generate images from random noise\n",
    "    gen_data = generator.predict([z, labels], verbose=0)\n",
    "\n",
    "    fake = np.ones((amount, 1)) * 0\n",
    "\n",
    "    real_fake = discriminator.predict([gen_data, labels], verbose=0)\n",
    "\n",
    "    # Rescale image pixel values to [0, 1]\n",
    "    gen_data = 0.5 * gen_data + 0.5\n",
    "\n",
    "    return gen_data, labels, real_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7f0b8f5-4070-4c64-9f44-7f188192d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data, labels, real_fake = sample_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "413b0a0a-f740-4acd-892b-9c62de35908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.normal(0, 1, (10 * 5, 256))\n",
    "label = np.arange(0, 5)\n",
    "labels = np.tile(label, 10).reshape(-1, 1)\n",
    "y_train = labels.squeeze()\n",
    "imgs = generator.predict([z, labels], verbose=0).reshape(50, 187)\n",
    "gen_imgs = generator.predict([z, labels], verbose=0).reshape(50, 187)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a08d3a3b-5fa6-41a5-b12e-00074e2e5cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e8fa284-086b-4670-88a7-db0b89b393e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: FID Scores {0: -6.920403901701413e-06, 1: -7.573819849466504e-06, 2: -1.3341462374954777e-05, 3: -3.0417200394114094e-06, 4: -8.41450704349659e-06}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "#  Compute FID for each label\n",
    "# -------------------------\n",
    "unique_labels = np.unique(y_train)\n",
    "fid_scores = {}\n",
    "\n",
    "for label in unique_labels:\n",
    "    real_samples = imgs[y_train == label]  # Select only real samples for the label\n",
    "    fake_samples = gen_imgs[y_train == label]  # Select only generated samples\n",
    "\n",
    "    if len(real_samples) > 1 and len(fake_samples) > 1:  # Ensure enough samples exist\n",
    "        fid_scores[label] = calculate_fid(real_samples, fake_samples)\n",
    "print(f\"Iteration: FID Scores {fid_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ccddef-2417-429a-aee0-112145b0f2e0",
   "metadata": {},
   "source": [
    "df_smooth = df_generated.drop(['label'], axis=1)\n",
    "df_smooth = df_smooth.apply(smooth_signal, axis=1, result_type=\"expand\")\n",
    "df_smooth['label'] = df_generated['label']\n",
    "df_smooth.columns = column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3276bba6-b560-40af-a6c1-7a76bc69af3a",
   "metadata": {},
   "source": [
    "fid = calculate_fid(df_train.drop(columns=[\"label\"]), df_smooth.drop(columns=[\"label\"]))\n",
    "fid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f676efc-3549-4c91-a6b0-966ad7f028a2",
   "metadata": {},
   "source": [
    "fid = calculate_fid(df_train.drop(columns=[\"label\"]), df_smooth.drop(columns=[\"label\"]))\n",
    "fid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e4fed-134f-4aa1-a69c-694ce6e8a3c7",
   "metadata": {},
   "source": [
    "fid = calculate_fid(df_train.drop(columns=[\"label\"]), df_smooth.drop(columns=[\"label\"]))\n",
    "fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e94dae-b07f-4486-9822-62551e1fcd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heart-beat",
   "language": "python",
   "name": "heart-beat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
